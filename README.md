# 100 Days of Deep Learning – Handwritten Learning Notes

These are my structured handwritten notes from the popular 
"100 Days of Deep Learning" playlist by CampusX (Nitish Singh).

I created these notes for revision, clarity, and concept reinforcement 
while going through the full series.

The journey is divided into three logical parts.

---

## Part 1/3 – Deep Learning Notes (Fundamentals)

Topics Covered:

• What is Deep Learning vs Machine Learning  
• Types & History of Neural Networks  
• Perceptron Intuition & Geometry  
• Perceptron Trick & Training  
• Loss Functions (Hinge Loss, Binary Cross Entropy)  
• Sigmoid Function  
• Limitations of Perceptron  
• MLP Notation  
• Multi-Layer Perceptron (MLP) Intuition  

---

## Part 2/3 – Deep Learning Notes (How Neural Networks Learn)

Topics Covered:

• Forward Propagation  
• Loss Functions in Deep Learning  
• Backpropagation  
• Gradient Descent and its Types  
• MLP Memoization  
• Vanishing Gradient Problem in ANN  

---

## Part 3/3 – Deep Learning Notes (Neural Network Optimization & Performance)

Topics Covered:

• Early Stopping  
• Data / Feature Scaling  
• Dropout Layers  
• Regularization (L1 / L2 / Weight Decay)  
• Activation Functions & ReLU Variants  
• Weight Initialization (Xavier / He)  
• Batch Normalization  
• Optimizers – SGD with Momentum, Nesterov Accelerated Gradient (NAG), 
  AdaGrad, RMSProp, Adam Optimizer  

---

## Bonus Notes

Some additional notes may include:

• Convolutional Neural Networks (CNN)  
• Transformers & Attention Mechanisms  

---

## Original Playlist

All concepts are based on:

100 Days of Deep Learning – CampusX

https://www.youtube.com/playlist?list=PLKnIA16_RmvYuZauWaPlRTC54KxSNLtNn

---

## Credits

All conceptual credit goes to CampusX and Nitish Singh.

These notes are compiled strictly for learning & revision purposes.

---

## Connect with Me

Feel free to connect with me on LinkedIn:

https://www.linkedin.com/in/tarun-negi-232258200/

---

